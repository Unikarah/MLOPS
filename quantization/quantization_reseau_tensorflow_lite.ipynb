{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "efc9100a06c04d0b8b17c557d7cbbd41",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Introduction à la quantization \n",
    "\n",
    "Laurent cetinsoy\n",
    "\n",
    "Les réseaux de neurones prennent beaucoup de place et il peut être difficile de les faire rentrer sur certains dispositifs embarqués. \n",
    "\n",
    "Il existe plusieurs méthodes pour réduire la taille et augmenter la vitesse d'executer des réseaux de neurone. Par exemple il y a ce qu'on appelle la quantization et le pruning.\n",
    "\n",
    "Dans ce notebook on va faire une introduction à la quantization avec la librairie tensorflow lite.\n",
    "\n",
    "\n",
    "## Quantization post training\n",
    "\n",
    "Dans un premier temps on va quantifier notre réseau après l'avoir entraîné normalement. \n",
    "\n",
    "\n",
    "Entraîner un réseau de neurone convolutionnel simple avec keras pour faire de la classification MNIST (ou un autre dataset simple de votre choix si (vous en avez marre de ce dataset - https://keras.io/api/datasets/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_id": "87bc3d9f649e416983c0ee9b59bd2de6",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 16:05:24.457143: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 169344000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 30s 68ms/step - loss: 0.3624 - accuracy: 0.8908 - val_loss: 0.0870 - val_accuracy: 0.9745\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 29s 68ms/step - loss: 0.1074 - accuracy: 0.9672 - val_loss: 0.0541 - val_accuracy: 0.9843\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 28s 67ms/step - loss: 0.0818 - accuracy: 0.9741 - val_loss: 0.0441 - val_accuracy: 0.9878\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 29s 70ms/step - loss: 0.0693 - accuracy: 0.9784 - val_loss: 0.0401 - val_accuracy: 0.9883\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 29s 68ms/step - loss: 0.0604 - accuracy: 0.9810 - val_loss: 0.0382 - val_accuracy: 0.9898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fedf443eda0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    " \n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b984ac621a2249d1b955535f9f9ab55b",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Afficher le nombre de paramètre du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_id": "11935acab177492cbcd4a8aafea0bdf0",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 13, 13, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 5, 5, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4782a0f186d14d4e91cfd903998f8710",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Sauvegarder votre modèle et afficher la taille du fichier. Si on applique une bête règle de trois, quelle est la taille occupée par paramètre ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "8f50426e286c4cf39610c41ed5cdfeb7",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/model_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/model_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du fichier contenant le modèle: 4096\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "saved_model_dir = \"saved_model\"\n",
    "\n",
    "model.save('saved_model/model_1')\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"Taille du fichier contenant le modèle:\", os.path.getsize('saved_model/model_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2e0f0d5c02dc40db9fae96aa67d4de06",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "On va maintenant convertir notre modèle keras en modèle tensorflow lite. \n",
    "\n",
    "Installer la librairie tensorflow lite créer une instance de la class TFLiteConverter à partir de votre modèle keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_id": "8c1f6d62ff6a416aa3ec3b05ebccddb9",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tflite in /home/unikarah/.local/lib/python3.10/site-packages (2.10.0)\n",
      "Requirement already satisfied: flatbuffers in /home/unikarah/.local/lib/python3.10/site-packages (from tflite) (22.11.23)\n",
      "Requirement already satisfied: numpy in /home/unikarah/.local/lib/python3.10/site-packages (from tflite) (1.23.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tflite\n",
    "\n",
    "import tflite\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "13a2cc6cc09f4e799d4e8641521ece8e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Convertir votre modèle et le sauvegarder dans un fichier nommé model.tflite. Sa taille est-elle plus petite ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_id": "2ca07a9b6ee74a6594c3a33313decfc0",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpi93lm8qj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpi93lm8qj/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du fichier contenant le modèle: 142360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 16:08:25.336561: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-11-30 16:08:25.336669: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-11-30 16:08:25.337077: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpi93lm8qj\n",
      "2022-11-30 16:08:25.340058: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-11-30 16:08:25.340253: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpi93lm8qj\n",
      "2022-11-30 16:08:25.359392: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-11-30 16:08:25.471254: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpi93lm8qj\n",
      "2022-11-30 16:08:25.514444: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 177371 microseconds.\n"
     ]
    }
   ],
   "source": [
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"saved_model/model.tflite\", \"wb\").write(tflite_model)\n",
    "print(\"Taille du fichier contenant le modèle:\", os.path.getsize('saved_model/model.tflite'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La taille est plus grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0e707074068f44ffad07b9301948782b",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "On va maintenant spécifier des optimisations au converter. \n",
    "\n",
    "1. Recréer un converter\n",
    "\n",
    "2. modifier son attribut optimizations pour ajouter une liste d'optimisation avec la valeur tf.lite.Optimize.DEFAULT\n",
    "\n",
    "3. Relancer la conversion du modèle, sauvegarder le modèle et regarder la taille du fichier généré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_id": "42b10a79262949aeadb6f2f82eae6f58",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbfu8wh87/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbfu8wh87/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du fichier contenant le modèle: 39944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 16:08:35.648852: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-11-30 16:08:35.648889: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-11-30 16:08:35.649007: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpbfu8wh87\n",
      "2022-11-30 16:08:35.650890: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-11-30 16:08:35.650967: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpbfu8wh87\n",
      "2022-11-30 16:08:35.657614: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-11-30 16:08:35.705922: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpbfu8wh87\n",
      "2022-11-30 16:08:35.729019: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 80012 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter_2 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter_2.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model_2 = converter_2.convert()\n",
    "\n",
    "open(\"saved_model/model_2.tflite\", \"wb\").write(tflite_model_2)\n",
    "print(\"Taille du fichier contenant le modèle:\", os.path.getsize('saved_model/model_2.tflite'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e083971b3c7647818e1532ed9c7edb41",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Quelle type  de quantization Optimize.Default, utilise-t-elle ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c4952d60d77243439193b6e9b4d573e9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "La quantization par défaut est la post-training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0bfeb7a588454ae6942059d7d79be609",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Quantization aware training \n",
    "\n",
    "Dans cette section on va s'intéresser à l'entraînement sensible à la quantification. L'idée est de simuler les effets de la quantification pendant l'entraînement pour que le modèle ajuste les poids afin de tenir ocmpte de la quantification. \n",
    "\n",
    "Reprendre le modèle entraîné sur MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cell_id": "11d35b87030b49eb99c77c4a6fe47db3",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "af90aac8bbb94f86b728031f5ae0a66f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "A l'aide de la fonction quantize de tensorflow_model_optimization, créer une seconde version de votre modèle entraîné nommé qat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_id": "3558b441b2b6461d8ada14a483cf14f6",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fedf4613100>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "qat_model = tfmot.quantization.keras.quantize_model(model)\n",
    "qat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "868beffe9e8740d98cbfd8458af4117a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Compiler le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cell_id": "a44bf47bba4746789745d44d3421a374",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": [],
   "source": [
    "qat_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4dddea973f404fb699b134a67d1a2dc5",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Afficher le summury du modèle. D'après vous ce modèle est-il quantifié ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "cell_id": "169400c0d68d417a8fc0acd42e3d8b4f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_5 (QuantizeL  (None, 28, 28, 1)        3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_conv2d_18 (QuantizeWr  (None, 26, 26, 32)       387       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_18 (Qua  (None, 13, 13, 32)       1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_conv2d_19 (QuantizeWr  (None, 11, 11, 64)       18627     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_19 (Qua  (None, 5, 5, 64)         1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_flatten_9 (QuantizeWr  (None, 1600)             1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dropout_9 (QuantizeWr  (None, 1600)             1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_9 (QuantizeWrap  (None, 10)               16015     \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,036\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 210\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "qat_model.summary()\n",
    "# On voit que les layers sont toutes mises comme quantizeLayer donc le modèle est quantifié"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7b3a873512174205b6d6f4aacd7480c8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Réentraîner votre modèle sur un sous ensemble des modèles sur une ou deux epochs et afficher la performance sur le train et test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "cell_id": "487c400c11454184b5d55729476d81c3",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "422/422 [==============================] - 44s 102ms/step - loss: 0.3816 - accuracy: 0.8835 - val_loss: 0.0811 - val_accuracy: 0.9785\n",
      "Epoch 2/2\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 0.1127 - accuracy: 0.9652 - val_loss: 0.0536 - val_accuracy: 0.9857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fedf47b0460>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model on subset of models\n",
    "qat_model.fit(x_train, y_train, batch_size=128, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 14ms/step - loss: 0.0285 - accuracy: 0.9899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02853892557322979, 0.9898999929428101]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display performance of model\n",
    "qat_model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cc4f49a0e7894b8385e90c2330a55bb1",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Convertir votre modèle avec TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cell_id": "68918464bfb14b749d3848fe25b5e02e",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, conv2d_19_layer_call_fn while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpmws1z2rt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpmws1z2rt/assets\n",
      "/home/unikarah/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2022-11-30 16:25:38.339623: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-11-30 16:25:38.339651: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-11-30 16:25:38.339871: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpmws1z2rt\n",
      "2022-11-30 16:25:38.347332: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-11-30 16:25:38.347364: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpmws1z2rt\n",
      "2022-11-30 16:25:38.386978: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-11-30 16:25:38.590778: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpmws1z2rt\n",
      "2022-11-30 16:25:38.664421: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 324550 microseconds.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m converter_qat\u001b[39m.\u001b[39moptimizations \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mlite\u001b[39m.\u001b[39mOptimize\u001b[39m.\u001b[39mDEFAULT]\n\u001b[1;32m      3\u001b[0m tflite_qat_model_2 \u001b[39m=\u001b[39m converter_qat\u001b[39m.\u001b[39mconvert()\n\u001b[0;32m----> 4\u001b[0m tflite_qat_model_2\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39m\u001b[39msaved_model/tflite_qat_model_2.tflite\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "converter_qat = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
    "converter_qat.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_qat_model_2 = converter_qat.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d80e6fbfea9646919a9cbe2c8fa41b2d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Comparer la performance du modèle Quantified aware training, au modèle original et au modèle quantifié post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cell_id": "bdd43405f5fd46ce892d957bb43a353a",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'evaluate_tflite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tflite_qat_model_2\u001b[39m.\u001b[39;49mevaluate_tflite(\u001b[39m'\u001b[39m\u001b[39msaved_model/tflite_qat_model_2.tflite\u001b[39m\u001b[39m'\u001b[39m, x_test, y_test)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'evaluate_tflite'"
     ]
    }
   ],
   "source": [
    "tflite_qat_model_2\n",
    "# FIXME: pb de loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6472c7a129394639838f76381e562588",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Sauvegarder le modèle QAT et comparer les tailles des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "cell_id": "d3c2951a1fd14be3b5e3c85e05a056ea",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du fichier contenant le modèle: 41896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "open(\"saved_model/tflite_qat_model_2.tflite\", \"wb\").write(tflite_qat_model_2)\n",
    "\n",
    "print(\"Taille du fichier contenant le modèle:\", os.path.getsize('saved_model/tflite_qat_model_2.tflite'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4a006a6d1a194e418072192da1a920de",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Bonus : déployer votre modèle sur votre téléphone ou un dispositif embarqué si vous en disposez d'un. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "927af747794641fa936ad727c6bf75d0",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e7635cb278ab43589a851e2c2de0d8ef",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Bonus : Obtenir un modèle qui sera à la fois quantifié et élagué (prunned) en s'aidant de la documentation (https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a1eacb94d0aa4dbd872c8babacf8bb8d",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0d51e245-899d-41d6-b23b-cf3e4bbbc6ea' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "fb1d23f975ba410e92fed9f5b8cbb7e6",
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
